{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QUESTIONS`\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example \n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ANSWERS`**Q1. What is the mathematical formula for a linear SVM?**\n",
    "\n",
    "The mathematical formula for a linear SVM is the decision function:\n",
    "\n",
    "\\[ f(x) = \\text{sign}(w \\cdot x + b) \\]\n",
    "\n",
    "where \\( x \\) is the input vector, \\( w \\) is the weight vector, and \\( b \\) is the bias term.\n",
    "\n",
    "**Q2. What is the objective function of a linear SVM?**\n",
    "\n",
    "The objective function of a linear SVM is to minimize the magnitude of the weight vector (\\(w\\)) to maximize the margin while ensuring correct classification. It is formulated as:\n",
    "\n",
    "\\[ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i(w \\cdot x_i + b) \\geq 1 \\, \\text{for} \\, i = 1, \\ldots, n \\]\n",
    "\n",
    "**Q3. What is the kernel trick in SVM?**\n",
    "\n",
    "The kernel trick involves implicitly mapping data into a higher-dimensional space using a kernel function, without explicitly calculating the transformation. This allows SVMs to handle non-linear relationships between features.\n",
    "\n",
    "**Q4. What is the role of support vectors in SVM? Explain with an example.**\n",
    "\n",
    "Support vectors are the data points crucial for defining the decision boundary and margin. They are either on the margin or misclassified. In a 2D space, consider two classes separated by a line. Support vectors are the points on or closest to the decision boundary.\n",
    "\n",
    "**Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM?**\n",
    "\n",
    "\n",
    "```\n",
    "# Q6: SVM Implementation through Iris dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Taking the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm_clf = SVC(kernel='linear', C=1)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', marker='o')\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = svm_clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(svm_clf.support_vectors_[:, 0], svm_clf.support_vectors_[:, 1], s=100,\n",
    "            linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.title('SVM Decision Boundaries')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
