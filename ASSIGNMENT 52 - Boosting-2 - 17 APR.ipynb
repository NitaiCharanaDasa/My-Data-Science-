{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is Gradient Boosting Regression?**\n",
    "Gradient Boosting Regression is an ensemble machine learning algorithm that builds a predictive model in the form of an ensemble of weak learners, typically decision trees. The algorithm sequentially fits new models to the residuals (the differences between the actual and predicted values) of the existing ensemble, adjusting the predictions at each step to minimize the overall error.\n",
    "\n",
    "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Define the Gradient Boosting Regression class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with the mean of the target variable\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update predictions using the tree's predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Save the tree\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], np.mean([tree.predict(X) for tree in self.trees], axis=0))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# Plot the true vs. predicted values\n",
    "plt.scatter(X_test, y_test, color='black', label='Actual')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. Experiment with different hyperparameters to optimize the performance of the model.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model's performance\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f'Mean Squared Error (Best Model): {mse_best}')\n",
    "print(f'R-squared (Best Model): {r2_best}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "\n",
    "**Q4. What is a weak learner in Gradient Boosting?**\n",
    "A weak learner in the context of Gradient Boosting is a model that performs slightly better than random chance on the given task. Typically, decision trees with limited depth are used as weak learners. These trees are called \"weak\" because they have low predictive power on their own and are prone to making errors. However, when combined in an ensemble through boosting, they contribute collectively to create a strong predictive model.\n",
    "\n",
    "**Q5. What is the intuition behind the Gradient Boosting algorithm?**\n",
    "The intuition behind Gradient Boosting is to sequentially build a series of weak learners, each focusing on correcting the errors made by the previous ones. The algorithm optimizes the model's predictions by minimizing the residual errors at each step. The key idea is to combine many weak models to create a strong, accurate model that can generalize well to new, unseen data.\n",
    "\n",
    "**Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?**\n",
    "Gradient Boosting builds an ensemble of weak learners by training each learner sequentially, with each new learner focusing on the errors made by the existing ensemble. The algorithm assigns weights to the weak learners based on their individual performance, and the final prediction is a weighted sum of the predictions made by all learners. The iterative training process ensures that the weak learners collectively contribute to minimizing the overall error.\n",
    "\n",
    "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**\n",
    "The mathematical intuition of Gradient Boosting involves the following steps:\n",
    "1. **Initialize the model:** Start with an initial model that predicts the mean of the target variable.\n",
    "2. **Compute residuals:** Calculate the residuals by subtracting the actual values from the predictions of the current model.\n",
    "3. **Train a weak learner:** Fit a weak learner (e.g., decision tree) to the residuals, with the goal of capturing the patterns in the data not explained by the current model.\n",
    "4. **Compute the learning rate-adjusted predictions:** Multiply the predictions of the weak learner by a learning rate (a hyperparameter between 0 and 1) to control the contribution of the weak learner to the final ensemble.\n",
    "5. **Update the model:** Update the current model by adding the learning rate-adjusted predictions of the weak learner.\n",
    "6. **Repeat:** Repeat steps 2-5 for a specified number of iterations or until a convergence criterion is met.\n",
    "7. **Final prediction:** The final prediction is the sum of the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
