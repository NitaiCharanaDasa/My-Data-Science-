{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QUESTONS`\n",
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ANSWERS`\n",
    "Q1: Ridge Regression is a linear regression technique that adds a penalty term, represented by the L2 norm of the coefficients, to the ordinary least squares regression. The penalty term helps prevent overfitting by discouraging large coefficient values.\n",
    "\n",
    "Q2: The assumptions of Ridge Regression are similar to those of ordinary least squares regression, including linearity, independence of errors, homoscedasticity, and normality of errors.\n",
    "\n",
    "Q3: The tuning parameter (lambda) in Ridge Regression is typically chosen through techniques like cross-validation, where different values of lambda are tested, and the one that provides the best model performance is selected.\n",
    "\n",
    "Q4: Yes, Ridge Regression can be used for feature selection to some extent. The penalty term in Ridge Regression can shrink less important features towards zero but doesn't set them exactly to zero. If strict feature selection is a primary goal, Lasso Regression may be more suitable.\n",
    "\n",
    "Q5: Ridge Regression performs well in the presence of multicollinearity, a situation where independent variables are highly correlated. The penalty term helps to stabilize the coefficients, making them less sensitive to collinearity.\n",
    "\n",
    "Q6: Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables may need to be appropriately encoded, such as through one-hot encoding.\n",
    "\n",
    "Q7: Coefficients in Ridge Regression are interpreted similarly to ordinary least squares regression. However, the penalty term introduces a trade-off: it shrinks coefficients towards zero, sacrificing some accuracy in individual predictors for improved model generalization.\n",
    "\n",
    "Q8: Ridge Regression can be used for time-series data analysis, but it may not be the first choice. Time-series data often involves serial correlation, and alternative techniques like autoregressive integrated moving average (ARIMA) or seasonal decomposition of time series (STL) may be more suitable. If using Ridge Regression, it's crucial to consider the temporal nature of the data and potential autocorrelation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
