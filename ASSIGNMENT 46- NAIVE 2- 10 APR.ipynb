{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **Probability of an employee being a smoker given the use of health insurance plan:**\n",
    "\n",
    "Let \\( S \\) be the event that an employee is a smoker, and \\( H \\) be the event that an employee uses the health insurance plan.\n",
    "\n",
    "\\[ P(S|H) = \\frac{P(H|S) \\cdot P(S)}{P(H)} \\]\n",
    "\n",
    "Given:\n",
    "- \\( P(H) = 0.70 \\) (probability of using health insurance plan)\n",
    "- \\( P(S) = 0.40 \\) (probability of being a smoker among those using the plan)\n",
    "- \\( P(H|S) = 1.0 \\) (probability of using the plan given that the employee is a smoker)\n",
    "\n",
    "\\[ P(S|H) = \\frac{1.0 \\cdot 0.40}{0.70} \\]\n",
    "\n",
    "\\[ P(S|H) \\approx 0.571 \\]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 57.1%.\n",
    "\n",
    "Q2. **Difference between Bernoulli Naive Bayes and Multinomial Naive Bayes:**\n",
    "\n",
    "- **Bernoulli Naive Bayes:** Assumes that features are binary (0 or 1) and is suitable for binary classification tasks. It models the presence or absence of features.\n",
    "\n",
    "- **Multinomial Naive Bayes:** Suitable for discrete data (e.g., word counts in text classification). It assumes that features are categorical and counts the occurrences of each category.\n",
    "\n",
    "Q3. **How Bernoulli Naive Bayes handles missing values:**\n",
    "\n",
    "Bernoulli Naive Bayes is generally designed for binary data. In scikit-learn, missing values (NaN) are treated as a separate category and are not explicitly handled. It is advisable to handle missing values before applying the classifier.\n",
    "\n",
    "Q4. **Can Gaussian Naive Bayes be used for multi-class classification?**\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It extends the binary classification to handle multiple classes by modeling the distribution of each feature for each class as a Gaussian (normal) distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. **Implementation Assignment:**\n",
    "\n",
    "Unfortunately, I can't execute code or create a Jupyter notebook in this text-based interface. However, I can guide you on how to implement the classifiers using scikit-learn.\n",
    "\n",
    "Here's a simplified code outline:\n",
    "\n",
    "```\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "names = [f\"feature_{i}\" for i in range(57)] + [\"is_spam\"]\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Split the data into features and target\n",
    "X = data.drop(\"is_spam\", axis=1)\n",
    "y = data[\"is_spam\"]\n",
    "\n",
    "# Instantiate classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Evaluate performance using 10-fold cross-validation\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring=\"accuracy\").mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring=\"precision\").mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring=\"recall\").mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring=\"f1\").mean()\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Report performance metrics\n",
    "accuracy_b, precision_b, recall_b, f1_b = evaluate_classifier(bernoulli_nb, X, y)\n",
    "accuracy_m, precision_m, recall_m, f1_m = evaluate_classifier(multinomial_nb, X, y)\n",
    "accuracy_g, precision_g, recall_g, f1_g = evaluate_classifier(gaussian_nb, X, y)\n",
    "\n",
    "# Print or store the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_b}\")\n",
    "print(f\"Precision: {precision_b}\")\n",
    "print(f\"Recall: {recall_b}\")\n",
    "print(f\"F1 Score: {f1_b}\")\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_m}\")\n",
    "print(f\"Precision: {precision_m}\")\n",
    "print(f\"Recall: {recall_m}\")\n",
    "print(f\"F1 Score: {f1_m}\")\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_g}\")\n",
    "print(f\"Precision: {precision_g}\")\n",
    "print(f\"Recall: {recall_g}\")\n",
    "print(f\"F1 Score: {f1_g}\")\n",
    "```\n",
    " **Discussion:**\n",
    "\n",
    "Discussing the results requires the actual execution of the code and examination of performance metrics. Generally, the choice between Bernoulli, Multinomial, or Gaussian Naive Bayes depends on the nature of the data. Bernoulli is suitable for binary data, Multinomial for discrete data, and Gaussian for continuous data.\n",
    "\n",
    " **Conclusion:**\n",
    "\n",
    "Summarize findings, discuss which variant performed best, and mention any limitations observed. Additionally, provide suggestions for future work, such as exploring advanced feature engineering or considering other classification algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
