{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`QUESTIONS` \n",
    "Q1: Ridge Regression is a linear regression technique that adds a penalty term, represented by the L2 norm of the coefficients, to the ordinary least squares regression. The penalty term helps prevent overfitting by discouraging large coefficient values.\n",
    "\n",
    "Q2: The assumptions of Ridge Regression are similar to those of ordinary least squares regression, including linearity, independence of errors, homoscedasticity, and normality of errors.\n",
    "\n",
    "Q3: The tuning parameter (lambda) in Ridge Regression is typically chosen through techniques like cross-validation, where different values of lambda are tested, and the one that provides the best model performance is selected.\n",
    "\n",
    "Q4: Yes, Ridge Regression can be used for feature selection to some extent. The penalty term in Ridge Regression can shrink less important features towards zero but doesn't set them exactly to zero. If strict feature selection is a primary goal, Lasso Regression may be more suitable.\n",
    "\n",
    "Q5: Ridge Regression performs well in the presence of multicollinearity, a situation where independent variables are highly correlated. The penalty term helps to stabilize the coefficients, making them less sensitive to collinearity.\n",
    "\n",
    "Q6: Yes, Ridge Regression can handle both categorical and continuous independent variables. Categorical variables may need to be appropriately encoded, such as through one-hot encoding.\n",
    "\n",
    "Q7: Coefficients in Ridge Regression are interpreted similarly to ordinary least squares regression. However, the penalty term introduces a trade-off: it shrinks coefficients towards zero, sacrificing some accuracy in individual predictors for improved model generalization.\n",
    "\n",
    "Q8: Ridge Regression can be used for time-series data analysis, but it may not be the first choice. Time-series data often involves serial correlation, and alternative techniques like autoregressive integrated moving average (ARIMA) or seasonal decomposition of time series (STL) may be more suitable. If using Ridge Regression, it's crucial to consider the temporal nature of the data and potential autocorrelation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ANSWERS`\n",
    "Q1: Lasso Regression is a linear regression technique that adds a penalty term, represented by the L1 norm of the coefficients, to the ordinary least squares regression. It differs from other regression techniques by encouraging sparsity in the coefficients, effectively leading to feature selection.\n",
    "\n",
    "Q2: The main advantage of Lasso Regression in feature selection is its ability to force some coefficients to be exactly zero. This results in a more interpretable and potentially simpler model by excluding irrelevant features.\n",
    "\n",
    "Q3: Coefficients in a Lasso Regression model are interpreted similarly to ordinary least squares regression. However, due to the penalty term, some coefficients may be exactly zero, indicating that the corresponding features are not contributing to the model.\n",
    "\n",
    "Q4: The main tuning parameter in Lasso Regression is the regularization parameter (lambda or alpha). It controls the strength of the penalty term. Larger values of lambda result in stronger regularization, potentially leading to more coefficients being exactly zero.\n",
    "\n",
    "Q5: Lasso Regression is primarily designed for linear regression problems. While it may capture some non-linear relationships, for more complex non-linear problems, other techniques like polynomial regression or non-linear models may be more suitable.\n",
    "\n",
    "Q6: The main difference between Ridge Regression and Lasso Regression lies in the penalty term. Ridge uses the L2 norm, while Lasso uses the L1 norm. Ridge tends to shrink coefficients towards zero, but they rarely become exactly zero, while Lasso can force some coefficients to be exactly zero, leading to feature selection.\n",
    "\n",
    "Q7: Yes, Lasso Regression can handle multicollinearity to some extent. The penalty term tends to shrink and select groups of correlated variables together. It may set one of the correlated variables to zero, effectively handling multicollinearity.\n",
    "\n",
    "Q8: The optimal value of the regularization parameter (lambda) in Lasso Regression is often chosen through techniques like cross-validation. Different values of lambda are tested, and the one that provides the best model performance (minimizing prediction error) is selected. Cross-validation helps find a balance between fitting the data well and maintaining model simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
